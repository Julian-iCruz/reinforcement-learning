{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ReinforcementLearning.GridWorld import GridWorld\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(**kwargs)\n",
    "        self.board = np.full(self.dimension, ' ')\n",
    "        self.actions = self.getActions()\n",
    "        self.qtable = {}\n",
    "        self.explored = 0\n",
    "        self.exploited = 0\n",
    "        self.generateBoard()\n",
    "        self.initialQtable()\n",
    "    \n",
    "    def generateBoard(self):\n",
    "        for key, value in self.rewards.items():\n",
    "            self.board[key] = value\n",
    "        \n",
    "        for key, value in self.walls.items():\n",
    "            self.board[key] = value\n",
    "\n",
    "    def printBoard(self):\n",
    "        for x in self.board:\n",
    "            print('\\n' + '------' * self.dimension[0])\n",
    "            for index, y in enumerate(x):\n",
    "                print(f'| {y} ', end='')\n",
    "                if index == self.dimension[0]:\n",
    "                    print('|', end='')\n",
    "        print('\\n' + '------'  * self.dimension[0])\n",
    "    \n",
    "    def getCurrentState(self):\n",
    "        return self.current_state\n",
    "\n",
    "    def getPossibleActions(self, state):\n",
    "        possible_actions = []\n",
    "        x, y = state\n",
    "        if x > 0 and self.board[x-1][y] != '*':\n",
    "            possible_actions.append('U')\n",
    "        if x < self.dimension[0]-1 and self.board[x+1][y] != '*':\n",
    "            possible_actions.append('D')\n",
    "        if y > 0 and self.board[x][y-1] != '*':\n",
    "            possible_actions.append('L')\n",
    "        if y < self.dimension[1]-1 and self.board[x][y+1] != '*':\n",
    "            possible_actions.append('R')\n",
    "        return possible_actions\n",
    "\n",
    "    def getActions(self):\n",
    "        actions = {}\n",
    "        for index, value in np.ndenumerate(self.board):\n",
    "            actions[index] = self.getPossibleActions(index)\n",
    "        return actions\n",
    "    \n",
    "    def initialQtable(self):\n",
    "        self.qtable = {}\n",
    "        for state in self.actions:\n",
    "            self.qtable[state]={}\n",
    "            for move in self.actions[state]:\n",
    "                self.qtable[state][move]=0\n",
    "\n",
    "    def getRandomPolicy(self):\n",
    "        policy = {}\n",
    "        for state in self.actions:\n",
    "            policy[state] = np.random.choice(self.actions[state])\n",
    "        return policy\n",
    "\n",
    "    def printPolicy(self, policy):\n",
    "        line = \"\"\n",
    "        counter = 0\n",
    "        print('------'  * self.dimension[0])\n",
    "        for item in policy:\n",
    "            line += f\"| {policy[item]} \"\n",
    "            counter += 1\n",
    "            if counter > self.dimension[0]:\n",
    "                print(line, end = '|\\n')\n",
    "                print('------' * self.dimension[0])\n",
    "                counter = 0\n",
    "                line = \"\"\n",
    "        print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {'D': 0, 'R': 0}, (0, 1): {'D': 0, 'L': 0, 'R': 0}, (0, 2): {'D': 0, 'L': 0, 'R': 0}, (0, 3): {'D': 0, 'L': 0}, (1, 0): {'U': 0, 'D': 0, 'R': 0}, (1, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 3): {'U': 0, 'D': 0, 'L': 0}, (2, 0): {'U': 0, 'R': 0}, (2, 1): {'U': 0, 'L': 0, 'R': 0}, (2, 2): {'U': 0, 'L': 0, 'R': 0}, (2, 3): {'U': 0, 'L': 0}}\n",
      "\n",
      "------------------\n",
      "|   |   |   | 1 |\n",
      "------------------\n",
      "|   | * |   | - |\n",
      "------------------\n",
      "|   |   |   |   |\n",
      "------------------\n",
      "\n",
      "Policy\n",
      "------------------\n",
      "| D | D | L | L |\n",
      "------------------\n",
      "| R | D | D | U |\n",
      "------------------\n",
      "| R | R | R | U |\n",
      "------------------\n",
      "\n",
      "{(0, 0): {'D': 0, 'R': 0}, (0, 1): {'D': 0, 'L': 0, 'R': 0}, (0, 2): {'D': 0, 'L': 0, 'R': 0}, (0, 3): {'D': 0, 'L': 0}, (1, 0): {'U': 0, 'D': 0, 'R': 0}, (1, 1): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 2): {'U': 0, 'D': 0, 'L': 0, 'R': 0}, (1, 3): {'U': 0, 'D': 0, 'L': 0}, (2, 0): {'U': 0, 'R': 0}, (2, 1): {'U': 0, 'L': 0, 'R': 0}, (2, 2): {'U': 0, 'L': 0, 'R': 0}, (2, 3): {'U': 0, 'L': 0}}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[376], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     estimatedQ \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(enviroment\u001b[39m.\u001b[39mqTable)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m \n\u001b[0;32m---> 19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(monte_carlo,\u001b[39mrange\u001b[39;49m(\u001b[39m1001\u001b[39;49m)))\n\u001b[1;32m     20\u001b[0m x\n",
      "Cell \u001b[0;32mIn[376], line 16\u001b[0m, in \u001b[0;36mmonte_carlo\u001b[0;34m(iteracion)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmonte_carlo\u001b[39m(iteracion):\n\u001b[0;32m---> 16\u001b[0m     estimatedQ \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(enviroment\u001b[39m.\u001b[39mqTable)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "env_vars = {\n",
    "    \"rewards\":{(0, 3): '1', (1, 3): '-'},\n",
    "    \"walls\":{(1,1): '*'},\n",
    "    \"dimension\":(3,4),\n",
    "    \"initial_state\":(0,0)\n",
    "}\n",
    "\n",
    "enviroment = GridWorld(**env_vars)\n",
    "policy = enviroment.getRandomPolicy()\n",
    "enviroment.printBoard()\n",
    "print('\\nPolicy')\n",
    "enviroment.printPolicy(policy)\n",
    "print(enviroment.qtable)\n",
    "\n",
    "def monte_carlo(iteracion):\n",
    "    estimatedQ = copy.deepcopy(enviroment.qTable)\n",
    "    return \n",
    "\n",
    "x = list(map(monte_carlo,range(1001)))\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
